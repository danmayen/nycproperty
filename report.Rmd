---
title: "New York City Property Sales - Harvard Data Science Project"
author: "Dr Daniel Mayenberger"
date: "May 2020"
header-includes:
   - \usepackage{hyperref}
   - \usepackage[usenames,dvipsnames]{xcolor}
   - \usepackage{amsmath}
   - \usepackage{tabularx}
output: 
    pdf_document:
        number_sections: true
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries and load data, include = FALSE}
# libraries
library(tidyverse)
library(ggplot2)
library(caret)
library(readr)
library(purrr)
library(lubridate)
# Set to TRUE for final version, is kept at FALSE for quicker compiling
# of drafts
FINAL_VERSION = FALSE

# load data from local repository for draft
if(FINAL_VERSION) {
    url <- "https://raw.githubusercontent.com/danmayen/nycproperty/master/
    data/nyc-property-sales.zip"
    # download file from url
    tmp_filename <- tempfile()
    csv_filename <- "nyc-rolling-sales.csv"
    download.file(url, tmp_filename)
    # unzip file
    unzip(tmp_filename, csv_filename)
    # remove temporary zipfile
    file.remove(tmp_filename)
    
    # read csv file into data frame
    nycproperty_raw <- read_csv(csv_filename)
    # remove csv file
    file.remove(csv_filename)
    
    # column names with underscores and lowercase
    newcolnames <- colnames(nycproperty_raw) %>%
        str_to_lower() %>%
        str_replace_all(" ", "_") %>%
        str_replace_all("-", "")
    colnames(nycproperty_raw) <- newcolnames

    # remove temporary variables
    rm(url, tmp_filename, csv_filename, newcolnames)
} else {
    load(file = "data/nycproperty_raw.Rdata")
    }
```

\newcommand{\blueref}[2]{\href{#1}{\textcolor{blue}{#2}}}
\newcommand{\greenref}[1]
{\color{green}\underline{{\color{black}\autoref{#1}}}\color{black}{}}
\newcommand{\greenuline}[1]
{\color{green}\underline{{\color{black}#1}}\color{black}{}}

# Executive Summary
There are thousands of real estate property transactions in New York City every
year. Based on the transactions of one year from
`r format(min(nycproperty_raw$sale_date), "%d %b %Y")` to
`r format(max(nycproperty_raw$sale_date), "%d %b %Y")`, we investigate 
algorithms that predict sale prices based on attributes of these 
properties.

The best-performing algorithm based on the root-mean squared error of the 
prices is

with an RMSE of [TBD] ([TBD] if including deed transfers)

# Introduction
The dataset of New York City property sales is sourced from the 
\blueref{https://www.kaggle.com/new-york-city/nyc-property-sales}
{Kaggle Website of NYC Property Sales}, as downloaded on 25 May 2020. The goal
is to predict the price of a property transaction based on the 20 other
attributes of the property that are available from the data set. The performance 
of the model is measured by the root-mean squared error (RMSE), defined as
\[
RMSE(m) = \sqrt{\frac{1}{N} 
\sum_{i=1}^N \left( \hat v_i(m) - v_i \right)^2},
\]

where $N$ is the total number of observations, $\hat v_i(m)$ is the 
predicted sale price of property $i$ under model $m$ and $v_i$ the actual
sale price.

To do so, the data are examined and modelled in 
\greenref{methods} which is further broken down into:

* \greenref{methods_techniques} to elaborate on the techniques used, in 
particular those coded in the later modelling \greenref{methods_modelling}.
* \greenref{methods_data_structure} to provide an overview of the basic
structure of the rating data.
* \greenref{methods_data_cleaning} to perform data cleaning.
* \greenref{methods_data_exp_vis} to visualise the most important 
properties [TBD]. These properties then inspire the modelling methods
in the subseqeuent section.
* \greenref{methods_modelling} presents the models based on the most
salient data properties and calibrates any free modelling parameters.

The results of all models are summarised in \greenref{results} and the
conclusions are drawn in \greenref{conclusion}.

# Methods and Analysis {#methods}

## Process and Techniques Used {#methods_techniques}

[TBD] modelling techniques will be described, [TBD].

## Data Structure and Loading {#methods_data_structure}

### Data Download {#methods_data_download}
The raw data is loaded from the Github location at
\blueref{https://github.com/danmayen/nycproperty/tree/master/data}
{this location} with the code shown in the Appendix 
\hyperlink{code_load_data}{\greenuline{here}}. The only minimal transformation
applied is the change of the column names to remove hyphens ("-"), replace
white spaces with underscores ("_") and convert them to lower case. There are
overall `r nrow(nycproperty_raw)` data points and 21 features as well as 
the sale price for each property.


### Split into Training Set and Test Set
A first look at the raw data shows that the following columns can be removed:

* The variable `x1` is a counter.
* The `easement` variable only holds `NA` values.
* Apartment numbers are too granular as an attribute.
* Single address information is too granular as well.

The related code for these facts is:
```{r raw data facts}
# number of different entries in x1
length(unique(nycproperty_raw$x1))
# number of enries of `easement` that are not NA
sum(!is.na(nycproperty_raw$easement))
# different apartment numbers
length(unique(nycproperty_raw$apartment_number))
# different addresses
length(unique(nycproperty_raw$address))
```

Further data will not be removed as that would constitute modelling 
hindsight, wherease discarding of the above columns does not require any
modelling insights. With that, the data are split into a training and a test
set:
```{r raw data split, eval = FALSE}
# remove columns that are of no use:
nycproperty_raw <- nycproperty_raw %>%
    select(-x1, -easement, -apartment_number, -address)

# Test set will be 10% of overall data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = nycproperty_raw$sale_price, 
                                  times = 1, p = 0.1, list = FALSE)
train_nycp <- nycproperty_raw[-test_index,]
test_nycp <- nycproperty_raw[test_index,]

# save to file for report
save(train_nycp, test_nycp, file = "data/nycp_split.Rdata")

# discard temporary variables
rm(test_index)
```

For the purposes of this report, the data are loaded from the data file
`nycp_split.Rdata`:

```{r load raw data split}
load(file = "data/nycp_split.Rdata")
```





### Basic Data Structure {#method_data_basic}
[TBD: Table of head]

From the overall 17 data attributes and one result remaining, the following data
properties are elicited for each column:

\begin{table}[htbp] \centering
\begin{tabularx}{\textwidth}{| X | X | X |}
    \hline
    No. & Variable & Basic Properties \\ 
    \hline \hline
    1  &  \texttt{borough} & There are five boroughs \\ \hline
    2  &  \texttt{neighborhood} & TBD Property \\ \hline
    3  &  \texttt{building\_class\_category} & TBD property \\ \hline
    4  &  \texttt{tax\_class\_at\_present} & TBD Property \\ \hline 
    5  &  \texttt{block} & TBD Property \\ \hline 
    6  &  \texttt{lot} & TBD Property \\ \hline 
    7  &  \texttt{building\_class\_at\_present} & TBD Property \\ \hline 
    8  &  \texttt{zip\_code} & TBD Property \\ \hline 
    9  &  \texttt{residential\_units} & TBD Property \\ \hline 
    10  &  \texttt{commercial\_units} & TBD Property \\ \hline 
    11  &  \texttt{total\_units} & TBD Property \\ \hline 
    12  &  \texttt{land\_square\_feet} & TBD Property \\ \hline 
    13  &  \texttt{gross\_square\_feet} & TBD Property \\ \hline 
    14  &  \texttt{year\_built} & TBD Property \\ \hline 
    15  &  \texttt{tax\_class\_at\_time\_of\_sale} & TBD Property \\ \hline 
    16  &  \texttt{building\_class\_at\_time\_of\_sale} & TBD Property \\ \hline 
    17  &  \texttt{sale\_date} & Date between TBD and TBD \\ \hline 
    18  &  \texttt{sale\_price} & TBD Property \\ \hline 
\end{tabularx}
\end{table}


The details can be found subsequently in \greenref{data_basic_borough} to
\greenref{data_basic_saleprice}.

#### Borough {#data_basic_borough}

#### Neighbourhood

#### Building Class Category

#### Tax Class at Present

#### Block

#### Lot

#### Building Class at Present

#### ZIP Code

#### Number of Residential Units

#### Number of Commercial Units

#### Total Number of Units

#### Land Square Feet

#### Gross Square Feet

#### Year Built

#### Tax Class at Time of Sale

#### Building Class at Time of Sale

#### Sale Date {#data_basic_saledate}

#### Sale Price {#data_basic_saleprice}

[TBD: tidiness of data]

## Data Cleaning {#methods_data_cleaning}

The data cleaning is summarised in the table below

Further details can be found in [TBD] - [TBD].

## Data Exploration and Visualisation {#methods_data_exp_vis}

### General Data Distribution Properties
For better readability, the code pieces for the subsequent graphs in this Section
are displayed in \hyperlink{appendix_figures}{\greenuline{Appendix A}}.


### Borough {#data_borough}


### Data Col 2 {#data_2}


## Modelling Approach {#methods_modelling}
The training of model parameters is generally done using k-fold 
cross-validation. To do this, the `edx` data is split into a training and
a validation set $k$ times. 

Graphically, k-fold validation can be illustrated by $k$ subsequent splits of
the overall `edx` set into training sets (shown in blue) and validation sets
(shown in purple).

```{r kfold crossvalidation, echo = FALSE}
n_data <- 1000
validation_split <- data.frame(
    k = seq(1, 25, 1),
    C = seq(0, 24, 1) / 25 * n_data,
    B = rep(1/25 * n_data, time = 25))
validation_split <- validation_split %>%
    mutate(A = n_data - B - C)
validation_split <- validation_split %>%
    gather(set, value, -k) 

validation_split %>%
    ggplot(aes(x = k, y = value, fill = set)) +
    scale_fill_manual(values = c("#ACE5EE", "#FAF0BE", "#ACE5EE")) +
    theme(plot.margin = margin(0,0,0,0, "cm")) +
    geom_bar(stat = "identity", position = "stack", show.legend = FALSE) +
    labs(title = "Illustration of k-fold cross validation (k = 25)",
         y = "Values")
```
\definecolor{blizzardblue}{rgb}{0.67, 0.9, 0.93}
\definecolor{blond}{rgb}{0.98, 0.94, 0.75}

For example, the parameter [TBD] of the [TBD] method introduced in
[TBD:ref] is applied to the
\colorbox{blizzardblue}{training data} and the RMSE that results from that
particular values of $\lambda$ is evaluated on the \colorbox{blond}{validation
data}. This is done $k=25$ times to produce an estimate of the RMSE depending on
$\lambda$.

The **performance** of each algorithm is evaluated using the RMSE which is
implemented in the function `RMSE_price`:

```{r RMSE function}
RMSE_price <- function(pred_price, actual_price) {
    ifelse(length(pred_price) > 0,
           sqrt(mean((pred_price - actual_price)^2)),
           NA)
    }
```

### Constant Value


### Method 1


### Method 2



# Results {#results}
The RMSEs are estimated for each of the seven models presented in 
\greenref{methods_modelling} and collected in a table for comparison. All
values are computed to the full seven digits to show also marginal differences 
in performance. The code follows these steps for each method:

1. Calculate estimate according to the method on the [TBD] data set.
2. Compute RMSE for this estimate on the [TBD] data set.
3. Store results in table `rmse_results`.

The mathematical formulation of the model is shown in the following
overview:

\begin{table}[htbp] \centering
\begin{tabularx}{\textwidth}{| X | X |}
    \hline
    Method  & Model Formula \\ 
    \hline \hline
    [Model]  &  [Formula] \\ \hline
\end{tabularx}
\end{table}


# Conclusion {#conclusion}


\newpage
\fontsize{14}{16}\selectfont{\textbf{Appendix A - Code of Figures}}

\hypertarget{appendix_figures} To enhance readibility of the report, the code
for data download in [Section 3.2](methods_data_structure) and most figures in 
[Section 3.4](methods_data_exp_vis) is shown in this Appendix.

\fontsize{12}{14}\selectfont{}
\textbf{Data Download in Section 3.2 - Data Structure and Loading}
\hypertarget{code_load_data}
The code for data download in \greenref{methods_data_download}:
```{r code load data, eval = FALSE}
# link address
url <- "https://raw.githubusercontent.com/danmayen/nycproperty/master/
data/nyc-property-sales.zip"
# download file from url
tmp_filename <- tempfile()
csv_filename <- "nyc-rolling-sales.csv"
download.file(url, tmp_filename)
# unzip file
unzip(tmp_filename, csv_filename)
# remove temporary zipfile
file.remove(tmp_filename)

# read csv file into data frame
nycproperty_raw <- read_csv(csv_filename)
# remove csv file
file.remove(csv_filename)
# remove temporary variables
rm(url, tmp_filename, csv_filename)

# column names with underscores and lowercase
newcolnames <- colnames(nycproperty_raw) %>%
    str_to_lower() %>%
    str_replace_all(" ", "_") %>%
    str_replace_all("-", "")
colnames(nycproperty_raw) <- newcolnames

# save to one file for report
save(nycproperty_raw, file = "data/nycproperty_raw.Rdata")

# remove temporary variables
rm(url, tmp_filename, csv_filename, newcolnames)
```


