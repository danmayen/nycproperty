---
title: "New York City Property Sales - Harvard Data Science Project"
author: "Dr Daniel Mayenberger"
date: "May 2020"
header-includes:
   - \usepackage{hyperref}
   - \usepackage[usenames,dvipsnames]{xcolor}
   - \usepackage{amsmath}
   - \usepackage{tabularx}
output: 
    pdf_document:
        number_sections: true
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries and load data, include = FALSE}
# libraries
library(tidyverse)
library(lubridate)
library(caret)
library(gam)
library(ggplot2)
library(gridExtra)
# Set to TRUE for final version, is kept at FALSE for quicker compiling
# of drafts
FINAL_VERSION = FALSE

# load data from local repository for draft
if(FINAL_VERSION) {
    }
```

\newcommand{\blueref}[2]{\href{#1}{\textcolor{blue}{#2}}}
\newcommand{\greenref}[1]
{\color{green}\underline{{\color{black}\autoref{#1}}}\color{black}{}}
\newcommand{\greenuline}[1]
{\color{green}\underline{{\color{black}#1}}\color{black}{}}

# Executive Summary
TBD

# Introduction
TBD

To do so, the data are examined and modelled in 
\greenref{methods} which is further broken down into:

* \greenref{methods_techniques} to elaborate on the techniques used, in 
particular those coded in the later modelling \greenref{methods_modelling}.
* \greenref{methods_data_structure} to provide an overview of the basic
structure of the rating data.
* \greenref{methods_data_cleaning} to perform data cleaning.
* \greenref{methods_data_exp_vis} to visualise the most important 
properties [TBD]. These properties then inspire the modelling methods
in the subseqeuent section.
* \greenref{methods_modelling} presents the models based on the most
salient data properties and calibrates any free modelling parameters.

The results of all models are summarised in \greenref{results} and the
conclusions are drawn in \greenref{conclusion}.

# Methods and Analysis {#methods}

## Process and Techniques Used {#methods_techniques}

[TBD] modelling techniques will be described, [TBD].

## Data Structure and Loading {#methods_data_structure}
The raw data is provided in two sets, called `edx` and `validation`.


### Basic Data Structure
[TBD: Table of head]


[TBD: tidiness of data]

## Data Cleaning {#methods_data_cleaning}

## Data Exploration and Visualisation {#methods_data_exp_vis}

### General Data Distribution Properties
For better readability, the code pieces for the subsequent graphs in this Section
are displayed in \hyperlink{appendix_figures}{\greenuline{Appendix A}}.


### Data Col 1 {#data_1}


### Data Col 2 {#data_2}


## Modelling Approach {#methods_modelling}
The training of model parameters is generally done using k-fold 
cross-validation. To do this, the `edx` data is split into a training and
a validation set $k$ times. 

Graphically, k-fold validation can be illustrated by $k$ subsequent splits of
the overall `edx` set into training sets (shown in blue) and validation sets
(shown in purple).

```{r kfold crossvalidation, echo = FALSE}
n_data <- 1000
validation_split <- data.frame(
    k = seq(1, 25, 1),
    C = seq(0, 24, 1) / 25 * n_data,
    B = rep(1/25 * n_data, time = 25))
validation_split <- validation_split %>%
    mutate(A = n_data - B - C)
validation_split <- validation_split %>%
    gather(set, value, -k) 

validation_split %>%
    ggplot(aes(x = k, y = value, fill = set)) +
    scale_fill_manual(values = c("#ACE5EE", "#FAF0BE", "#ACE5EE")) +
    theme(plot.margin = margin(0,0,0,0, "cm")) +
    geom_bar(stat = "identity", position = "stack", show.legend = FALSE) +
    labs(title = "Illustration of k-fold cross validation (k = 25)",
         y = "Values")
```
\definecolor{blizzardblue}{rgb}{0.67, 0.9, 0.93}
\definecolor{blond}{rgb}{0.98, 0.94, 0.75}

For example, the parameter [TBD] of the [TBD] method introduced in
[TBD:ref] is applied to the
\colorbox{blizzardblue}{training data} and the RMSE that results from that
particular values of $\lambda$ is evaluated on the \colorbox{blond}{validation
data}. This is done $k=25$ times to produce an estimate of the RMSE depending on
$\lambda$.

The **performance** of each algorithm is evaluated using the RMSE which is
implemented in the function `RMSE_price`:

```{r RMSE function}
RMSE_rating <- function(pred_ratings, true_ratings) {
    ifelse(length(pred_ratings) > 0,
           sqrt(mean((pred_ratings - true_ratings)^2)),
           NA)
    }
```

### Constant Value


### Method 1


### Method 2



# Results {#results}
The RMSEs are estimated for each of the seven models presented in 
\greenref{methods_modelling} and collected in a table for comparison. All
values are computed to the full seven digits to show also marginal differences 
in performance. The code follows these steps for each method:

1. Calculate estimate according to the method on the [TBD] data set.
2. Compute RMSE for this estimate on the [TBD] data set.
3. Store results in table `rmse_results`.

The mathematical formulation of the model is shown in the following
overview:

\begin{table}[htbp] \centering
\begin{tabularx}{\textwidth}{| X | X |}
    \hline
    Method  & Model Formula \\ 
    \hline \hline
    [Model]  &  [Formula] \\ \hline
\end{tabularx}
\end{table}


# Conclusion {#conclusion}


\newpage
\fontsize{14}{16}\selectfont{\textbf{Appendix A - Code of Figures}}

\hypertarget{appendix_figures} To enhance readibility of the report, the code
for most figures in [Section 3.4](methods_data_exp_vis) is shown in this
Appendix.

\fontsize{12}{14}\selectfont{}
\textbf{Figures in Section 3.4.1 - General Data Distribution}

<!-- \hypertarget{label} -->
Code for [TBD] figure shown [TBD]:

